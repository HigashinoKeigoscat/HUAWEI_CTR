{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtox72csOQUN"
   },
   "source": [
    "# DeepMatch 样例代码\n",
    "- https://github.com/shenweichen/DeepMatch\n",
    "- https://deepmatch.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9UxNHuPMuW2"
   },
   "source": [
    "# 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=['age','gender' , 'residence' , 'city', 'city_rank', 'series dev', 'series group', 'emui dev','device name','device size', 'net_type']      \n",
    "item=['task id', 'adv id','creat_type_cd','adv_prim id', 'inter type cd','slot id', 'spread app id','app second class','log_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C_ZR6gzp1E2N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat\n",
    "from preprocess import gen_data_set, gen_model_input,gen_data_set_sdm,gen_model_input_sdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss, NegativeSampler\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import defaultdict  \n",
    "import os, math, warnings, math, pickle\n",
    "import faiss\n",
    "import collections\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat,DenseFeat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "warnings.filterwarnings('ignore')\n",
    "from deepmatch.utils import sampledsoftmaxloss, NegativeSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_ads = pd.read_pickle('d:/Data/2022_3_data/train/train_data_ads.pkl')\n",
    "train_ads[\"time_YMD\"]=train_ads[\"pt_d\"]//10000-20220603\n",
    "train_ads[\"hour\"]=train_ads[\"pt_d\"]%10000//100\n",
    "data=train_ads.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQq6O9XAMzPF"
   },
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lcO29zFb21Od",
    "outputId": "cda19a71-6a6e-4113-f42d-ab80f06273b8"
   },
   "outputs": [],
   "source": [
    "# data_path = \"./\"\n",
    "\n",
    "# unames = ['user_id','gender','age','occupation','zip']\n",
    "# user = pd.read_csv(data_path+'ml-1m/users.dat',sep='::',header=None,names=unames)\n",
    "# rnames = ['user_id','movie_id','rating','timestamp']\n",
    "# ratings = pd.read_csv(data_path+'ml-1m/ratings.dat',sep='::',header=None,names=rnames)\n",
    "# mnames = ['movie_id','title','genres']\n",
    "# movies = pd.read_csv(data_path+'ml-1m/movies.dat',sep='::',header=None,names=mnames,encoding=\"unicode_escape\")\n",
    "# movies['genres'] = list(map(lambda x: x.split('|')[0], movies['genres'].values))\n",
    "\n",
    "# data = pd.merge(pd.merge(ratings,movies),user)#.iloc[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0yCWxQxM3se"
   },
   "source": [
    "# 构建特征列，训练模型，导出embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log_id', 'label', 'user_id', 'age', 'gender', 'residence', 'city',\n",
       "       'city_rank', 'series_dev', 'series_group', 'emui_dev', 'device_name',\n",
       "       'device_size', 'net_type', 'task_id', 'adv_id', 'creat_type_cd',\n",
       "       'adv_prim_id', 'inter_type_cd', 'slot_id', 'site_id', 'spread_app_id',\n",
       "       'hispace_app_tags', 'app_second_class', 'app_score',\n",
       "       'ad_click_list_v001', 'ad_click_list_v002', 'ad_click_list_v003',\n",
       "       'ad_close_list_v001', 'ad_close_list_v002', 'ad_close_list_v003',\n",
       "       'pt_d', 'u_newsCatInterestsST', 'u_refreshTimes', 'u_feedLifeCycle',\n",
       "       'time_YMD', 'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ads.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data_set_sdm(data, seq_short_max_len=5, seq_prefer_max_len=50):\n",
    "    data.sort_values(\"pt_d\", inplace=True)\n",
    "    item_ids = data['adv_id'].unique()\n",
    "    item_id_genres_map = dict(zip(data['adv_id'].values, data['creat_type_cd'].values))\n",
    "    train_set, test_set = [], []\n",
    "    for user_id, hist_click in tqdm(data.groupby('user_id')):\n",
    "        # 这里按照expo_date分开，每一天用滑动窗口滑，可能相关性更高些,另外，这样序列不会太长，因为eda发现有点击1111个的\n",
    "        #for expo_date, hist_click in hist_date_click.groupby('expo_date'):\n",
    "        # 用户当天的点击历史id\n",
    "        pos_list = hist_click['adv_id'].tolist()\n",
    "        label_list=hist_click['label'].tolist()\n",
    "        pt_d_list=hist_click[\"pt_d\"].tolist()\n",
    "        age_list=hist_click[\"age\"].tolist()\n",
    "        gender_list=hist_click[\"gender\"].tolist()\n",
    "        residence_list=hist_click[\"residence\"].tolist()\n",
    "        city_list=hist_click[\"city\"].tolist()\n",
    "        city_rank_list=hist_click[\"city_rank\"].tolist()\n",
    "        series_dev_list=hist_click['series_dev'].tolist()\n",
    "        series_group_list=hist_click['series_group'].tolist()\n",
    "        emui_dev_list=hist_click['emui_dev'].tolist()\n",
    "        device_name_list=hist_click['device_name'].tolist()\n",
    "        device_size_list=hist_click['device_size'].tolist()\n",
    "        net_type_list=hist_click['net_type'].tolist()\n",
    "        task_id_list=hist_click[\"task_id\"].tolist()\n",
    "        create_type_list=hist_click[\"creat_type_cd\"].tolist()\n",
    "        adv_prim_list=hist_click[\"adv_prim_id\"].tolist()\n",
    "        inter_tyepe_list=hist_click[\"inter_type_cd\"].tolist()\n",
    "        slot_id_list=hist_click[\"slot_id\"].tolist()\n",
    "        spread_app_id_list=hist_click[\"spread_app_id\"].tolist()\n",
    "        app_second_id_list=hist_click[\"app_second_class\"].tolist()\n",
    "\n",
    "        #user_control_flag = True\n",
    "        \n",
    "        \n",
    "        # 过长的序列截断\n",
    "       \n",
    "        #if negsample > 0:\n",
    "        #    neg_list = gen_neg_sample_candiate(pos_list, item_ids, doc_clicked_count_dict, negsample, methods='multinomial')\n",
    "        \n",
    "        # 只有1个的也截断 去掉，当然我之前做了处理，这里没有这种情况了\n",
    "        if len(pos_list) < 1:\n",
    "            continue\n",
    "        else: \n",
    "            for i in range(0, len(pos_list)):\n",
    "                hist = pos_list[:i]\n",
    "                genres_hist = create_type_list[:i]\n",
    "                seq_short_len = min(i, seq_short_max_len)\n",
    "                seq_prefer_len = min(max(i - seq_short_len, 0), seq_prefer_max_len)\n",
    "                row = [user_id,  hist[::-1][:seq_short_len][::-1], pos_list[i],age_list[i],gender_list[i],residence_list[i],city_list[i]\n",
    "                ,city_rank_list[i],series_dev_list[i],series_group_list[i],emui_dev_list[i],device_name_list[i], \n",
    "                device_size_list[i],net_type_list[i],label_list[i],len(hist[::-1]),task_id_list[i],\n",
    "                create_type_list[i],adv_prim_list[i],inter_tyepe_list[i],slot_id_list[i],\n",
    "                spread_app_id_list[i],app_second_id_list[i],\n",
    "                hist[::-1][seq_short_len:seq_short_len + seq_prefer_len] ,seq_short_len,\n",
    "                     seq_prefer_len, genres_hist[::-1][:seq_short_len][::-1],\n",
    "                     genres_hist[::-1][seq_short_len:seq_short_len + seq_prefer_len],]\n",
    "                if pt_d_list[i]<202206070000:\n",
    "                    train_set.append(row)\n",
    "                else:\n",
    "                    test_set.append(row)\n",
    "# hist[::-1][:seq_len], seq_len, genres_hist[::-1][:seq_len],\n",
    "#                    genres_list[i],\n",
    "#                    rating_list[i]))\n",
    "              \n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "    \n",
    "    return train_set, test_set   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_id_list=hist_click[\"task_id\"].tolist()\n",
    "#         create_type_list=hist_click[\"creat_type_cd\"].tolist()\n",
    "#         adv_prim_list=hist_click[\"adv_prim_id\"].tolist()\n",
    "#         inter_tyepe_list=hist_click[\"inter_type_cd\"].tolist()\n",
    "#         slot_id_list=hist_click[\"slot_id\"].tolist()\n",
    "#         spread_app_id_list=hist_click[\"spread_app_id\"].tolist()\n",
    "#         app_second_id_list=hist_click[\"app_second_id\"].tolist()\n",
    "#         log_id_list=hist_click[\"log_id\"].tolist()\n",
    "\n",
    "def gen_model_input_sdm(train_set, user_profile, seq_short_max_len, seq_prefer_max_len):\n",
    "    train_uid = np.array([row[0] for row in train_set])\n",
    "    short_train_seq = [row[1] for row in train_set]\n",
    "    prefer_train_seq = [line[23] for line in train_set]\n",
    "    train_iid = np.array([row[2] for row in train_set])\n",
    "    train_u_age = np.array([row[3] for row in train_set])\n",
    "    train_u_gender = np.array([row[4] for row in train_set])\n",
    "    train_u_residence=np.array([row[5] for row in train_set])\n",
    "    train_u_city = np.array([row[6] for row in train_set])\n",
    "    train_u_city_rank = np.array([row[7] for row in train_set])\n",
    "    train_u_series_dev=np.array([row[8] for row in train_set])\n",
    "    train_u_series_group=np.array([row[9] for row in train_set])\n",
    "    train_u_emui_dev=np.array([row[10] for row in train_set])\n",
    "    train_u_device_name = np.array([row[11] for row in train_set])\n",
    "    train_u_device_size = np.array([row[12] for row in train_set])\n",
    "    train_u_net_type_list=np.array([row[13] for row in train_set])\n",
    "    train_label = np.array([row[14] for row in train_set])\n",
    "    train_hist_len = np.array([row[15] for row in train_set])\n",
    "    \n",
    "    train_task_id=np.array([row[16]for row in train_set])\n",
    "    train_create_type=np.array([row[17] for row in train_set])\n",
    "    train_adv_prim=np.array([row[18] for row in train_set])\n",
    "    train_inter_tyepe_list=np.array([row[19] for row in train_set])\n",
    "    train_slot_id_list=np.array([row[20] for row in train_set])\n",
    "    train_spread_app_id_list=np.array([row[21] for row in train_set])\n",
    "    train_app_second_id_list=np.array([row[22] for row in train_set])\n",
    "    \n",
    "    train_short_len = np.array([line[24] for line in train_set])\n",
    "    train_prefer_len = np.array([line[25] for line in train_set])\n",
    "    short_train_seq_genres = np.array([line[26] for line in train_set])\n",
    "    prefer_train_seq_genres = np.array([line[27] for line in train_set])\n",
    "\n",
    "\n",
    "    train_short_item_pad = pad_sequences(short_train_seq, maxlen=seq_short_max_len, padding='post', truncating='post',\n",
    "                                         value=0)\n",
    "    train_prefer_item_pad = pad_sequences(prefer_train_seq, maxlen=seq_prefer_max_len, padding='post',\n",
    "                                          truncating='post',\n",
    "                                          value=0)\n",
    "    train_short_genres_pad = pad_sequences(short_train_seq_genres, maxlen=seq_short_max_len, padding='post',\n",
    "                                           truncating='post',\n",
    "                                           value=0)\n",
    "    train_prefer_genres_pad = pad_sequences(prefer_train_seq_genres, maxlen=seq_prefer_max_len, padding='post',\n",
    "                                            truncating='post',\n",
    "                                            value=0)\n",
    "    \n",
    "    train_model_input = {\n",
    "        \"user_id\": train_uid,\n",
    "        \"adv_id\": train_iid,\n",
    "        \"short_movie_id\": train_short_item_pad,\n",
    "        \"prefer_movie_id\": train_prefer_item_pad,\n",
    "        \"hist_len\": train_hist_len,\n",
    "        \"prefer_sess_length\": train_prefer_len,\n",
    "        \"short_sess_length\": train_short_len, \n",
    "        'short_genres': train_short_genres_pad,\n",
    "        'prefer_genres': train_prefer_genres_pad,\n",
    "        \"u_city\": train_u_city,\n",
    "        \"u_age\": train_u_age,\n",
    "        \"u_gender\": train_u_gender,\n",
    "        \"residence\": train_u_residence,\n",
    "        \"city_rank\":train_u_city_rank,\n",
    "        \"series_dev\":train_u_series_dev,\n",
    "        \"series_group\":train_u_series_group,\n",
    "        \"emui_dev\":train_u_emui_dev,\n",
    "        \"device_name\":train_u_device_name,\n",
    "        \"device_size\":train_u_device_size,\n",
    "        \"net_type_list\":train_u_net_type_list,\n",
    "        \n",
    "        \"task_id\":train_task_id,\n",
    "        \"create_type\":train_create_type,\n",
    "        \"adv_prim\":train_adv_prim,\n",
    "        \"inter_tyepe\":train_inter_tyepe_list,\n",
    "        \"slot_id\":train_slot_id_list,\n",
    "        \"spread_app_id\":train_spread_app_id_list,\n",
    "        \"pp_second_id\":train_app_second_id_list\n",
    "    }\n",
    "\n",
    "   \n",
    "\n",
    "    # train_model_input = {\"user_id\": train_uid, \"movie_id\": train_iid, \"short_movie_id\": train_short_item_pad,\n",
    "    #                      \"prefer_movie_id\": train_prefer_item_pad,\n",
    "    #                      \"prefer_sess_length\": train_prefer_len,\n",
    "    #                      \"short_sess_length\": train_short_len, 'short_genres': train_short_genres_pad,\n",
    "    #                      'prefer_genres': train_prefer_genres_pad}\n",
    "\n",
    "    # for key in [\"gender\", \"age\", \"occupation\", \"zip\"]:\n",
    "    #     train_model_input[key] = user_profile.loc[train_model_input['user_id']][key].values\n",
    "\n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " #获取用户embedding和文章embedding\n",
    "def get_embeddings(model, test_model_input, user_idx_2_rawid, doc_idx_2_rawid, save_path='d:/Data/2022_3_data/train/'):\n",
    "    doc_model_input = {'adv_id':np.array(list(doc_idx_2_rawid.keys()))}\n",
    "    \n",
    "    user_embedding_model = Model(inputs=model.user_input, outputs=model.user_embedding)\n",
    "    doc_embedding_model = Model(inputs=model.item_input, outputs=model.item_embedding)\n",
    "    \n",
    "    # 保存当前的item_embedding 和 user_embedding 排序的时候可能能够用到，但是需要注意保存的时候需要和原始的id对应\n",
    "    user_embs = user_embedding_model.predict(test_model_input, batch_size=2 ** 12)\n",
    "    doc_embs = doc_embedding_model.predict(doc_model_input, batch_size=2 ** 12)\n",
    "    # embedding保存之前归一化一下\n",
    "    user_embs = user_embs / np.linalg.norm(user_embs, axis=1, keepdims=True)\n",
    "    doc_embs = doc_embs / np.linalg.norm(doc_embs, axis=1, keepdims=True)\n",
    "    \n",
    "    # 将Embedding转换成字典的形式方便查询\n",
    "    raw_user_id_emb_dict = {user_idx_2_rawid[k]: \\\n",
    "                                v for k, v in zip(user_idx_2_rawid.keys(), user_embs)}\n",
    "    raw_doc_id_emb_dict = {doc_idx_2_rawid[k]: \\\n",
    "                                v for k, v in zip(doc_idx_2_rawid.keys(), doc_embs)}\n",
    "    # 将Embedding保存到本地\n",
    "    pickle.dump(raw_user_id_emb_dict, open(save_path + 'user_youtube_emb.pkl', 'wb'))\n",
    "    pickle.dump(raw_doc_id_emb_dict, open(save_path + 'doc_youtube_emb.pkl', 'wb'))\n",
    "    \n",
    "    # 读取\n",
    "    #user_embs_dict = pickle.load(open('embedding/user_youtube_emb.pkl', 'rb'))\n",
    "    #doc_embs_dict = pickle.load(open('embedding/doc_youtube_emb.pkl', 'rb'))\n",
    "    return raw_user_id_emb_dict, raw_doc_id_emb_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log_id', 'label', 'user_id', 'age', 'gender', 'residence', 'city',\n",
       "       'city_rank', 'series_dev', 'series_group', 'emui_dev', 'device_name',\n",
       "       'device_size', 'net_type', 'task_id', 'adv_id', 'creat_type_cd',\n",
       "       'adv_prim_id', 'inter_type_cd', 'slot_id', 'site_id', 'spread_app_id',\n",
       "       'hispace_app_tags', 'app_second_class', 'app_score',\n",
       "       'ad_click_list_v001', 'ad_click_list_v002', 'ad_click_list_v003',\n",
       "       'ad_close_list_v001', 'ad_close_list_v002', 'ad_close_list_v003',\n",
       "       'pt_d', 'u_newsCatInterestsST', 'u_refreshTimes', 'u_feedLifeCycle',\n",
       "       'time_YMD', 'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ads.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 913
    },
    "id": "BMOvk_de2ML3",
    "outputId": "eba1ad5c-7a45-4b30-84f6-0d19f556834c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 12751/65297 [00:21<01:27, 597.77it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zhouda\\Downloads\\DeepMatch-master\\examples\\colab_MovieLen1M_SDM.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m user_profile\u001b[39m.\u001b[39mset_index(\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m user_item_list \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mgroupby(\u001b[39m\"\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39madv_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlist\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m train_set, test_set \u001b[39m=\u001b[39m gen_data_set_sdm(data, seq_short_max_len\u001b[39m=\u001b[39;49mSEQ_LEN_short, seq_prefer_max_len\u001b[39m=\u001b[39;49mSEQ_LEN_prefer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m train_model_input, train_label \u001b[39m=\u001b[39m gen_model_input_sdm(train_set, user_profile, SEQ_LEN_short, SEQ_LEN_prefer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m test_model_input, test_label \u001b[39m=\u001b[39m gen_model_input_sdm(test_set, user_profile, SEQ_LEN_short, SEQ_LEN_prefer)\n",
      "\u001b[1;32mc:\\Users\\zhouda\\Downloads\\DeepMatch-master\\examples\\colab_MovieLen1M_SDM.ipynb Cell 14\u001b[0m in \u001b[0;36mgen_data_set_sdm\u001b[1;34m(data, seq_short_max_len, seq_prefer_max_len)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m item_id_genres_map \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(data[\u001b[39m'\u001b[39m\u001b[39madv_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues, data[\u001b[39m'\u001b[39m\u001b[39mcreat_type_cd\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_set, test_set \u001b[39m=\u001b[39m [], []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m user_id, hist_click \u001b[39min\u001b[39;00m tqdm(data\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# 这里按照expo_date分开，每一天用滑动窗口滑，可能相关性更高些,另外，这样序列不会太长，因为eda发现有点击1111个的\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m#for expo_date, hist_click in hist_date_click.groupby('expo_date'):\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# 用户当天的点击历史id\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     pos_list \u001b[39m=\u001b[39m hist_click[\u001b[39m'\u001b[39m\u001b[39madv_id\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zhouda/Downloads/DeepMatch-master/examples/colab_MovieLen1M_SDM.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     label_list\u001b[39m=\u001b[39mhist_click[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:708\u001b[0m, in \u001b[0;36mBaseGrouper.get_iterator\u001b[1;34m(self, data, axis)\u001b[0m\n\u001b[0;32m    706\u001b[0m splitter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_splitter(data, axis\u001b[39m=\u001b[39maxis)\n\u001b[0;32m    707\u001b[0m keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroup_keys_seq\n\u001b[1;32m--> 708\u001b[0m \u001b[39mfor\u001b[39;00m key, group \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(keys, splitter):\n\u001b[0;32m    709\u001b[0m     \u001b[39myield\u001b[39;00m key, group\u001b[39m.\u001b[39m__finalize__(data, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:1233\u001b[0m, in \u001b[0;36mDataSplitter.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1230\u001b[0m starts, ends \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mgenerate_slices(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslabels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngroups)\n\u001b[0;32m   1232\u001b[0m \u001b[39mfor\u001b[39;00m start, end \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(starts, ends):\n\u001b[1;32m-> 1233\u001b[0m     \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_chop(sdata, \u001b[39mslice\u001b[39;49m(start, end))\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:1258\u001b[0m, in \u001b[0;36mFrameSplitter._chop\u001b[1;34m(self, sdata, slice_obj)\u001b[0m\n\u001b[0;32m   1252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chop\u001b[39m(\u001b[39mself\u001b[39m, sdata: DataFrame, slice_obj: \u001b[39mslice\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m   1253\u001b[0m     \u001b[39m# Fastpath equivalent to:\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m     \u001b[39m# if self.axis == 0:\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m     \u001b[39m#     return sdata.iloc[slice_obj]\u001b[39;00m\n\u001b[0;32m   1256\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m   1257\u001b[0m     \u001b[39m#     return sdata.iloc[:, slice_obj]\u001b[39;00m\n\u001b[1;32m-> 1258\u001b[0m     mgr \u001b[39m=\u001b[39m sdata\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mget_slice(slice_obj, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maxis)\n\u001b[0;32m   1259\u001b[0m     \u001b[39m# __finalize__ not called here, must be applied by caller if applicable\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m     \u001b[39mreturn\u001b[39;00m sdata\u001b[39m.\u001b[39m_constructor(mgr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#data = pd.read_csvdata = pd.read_csv(\"./movielens_sample.txt\")\n",
    "sparse_features=['time_YMD','hour', 'task_id','adv_id', 'creat_type_cd', 'user_id','age', 'gender','residence','city_rank', 'series_dev','series_group','emui_dev','adv_prim_id','device_name','device_size','city','net_type','inter_type_cd','slot_id', 'spread_app_id', 'app_second_class' ]\n",
    "SEQ_LEN = 50\n",
    "SEQ_LEN_short = 5\n",
    "SEQ_LEN_prefer = 50\n",
    "negsample = 0\n",
    "\n",
    "# 1.Label Encoding for sparse features,and process sequence features with `gen_date_set` and `gen_model_input`\n",
    "\n",
    "feature_max_idx = {}\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature]) + 1\n",
    "    feature_max_idx[feature] = data[feature].max() + 1\n",
    "features = sparse_features\n",
    "user_profile = data[features].drop_duplicates('user_id')\n",
    "\n",
    "item_profile = data[[\"adv_id\"]].drop_duplicates('adv_id')\n",
    "\n",
    "user_profile.set_index(\"user_id\", inplace=True)\n",
    "\n",
    "user_item_list = data.groupby(\"user_id\")['adv_id'].apply(list)\n",
    "\n",
    "train_set, test_set = gen_data_set_sdm(data, seq_short_max_len=SEQ_LEN_short, seq_prefer_max_len=SEQ_LEN_prefer)\n",
    "\n",
    "train_model_input, train_label = gen_model_input_sdm(train_set, user_profile, SEQ_LEN_short, SEQ_LEN_prefer)\n",
    "test_model_input, test_label = gen_model_input_sdm(test_set, user_profile, SEQ_LEN_short, SEQ_LEN_prefer)\n",
    "\n",
    "# 2.count #unique features for each sparse field and generate feature config for sequence feature\n",
    "\n",
    "embedding_dim = 32\n",
    "user_id_raw = data[['user_id']].drop_duplicates('user_id',keep=\"last\")\n",
    "doc_id_raw = data[['adv_id']].drop_duplicates('adv_id',keep=\"last\")\n",
    "user_id_enc = data[['user_id']].drop_duplicates('user_id',keep=\"last\")\n",
    "doc_id_enc = data[['adv_id']].drop_duplicates('adv_id',keep=\"last\")\n",
    "user_idx_2_rawid = dict(zip(user_id_enc['user_id'], user_id_raw['user_id']))\n",
    "doc_idx_2_rawid = dict(zip(doc_id_enc['adv_id'], doc_id_raw['adv_id']))\n",
    "user_feature_columns = [\n",
    "    SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),\n",
    "    SparseFeat('u_city', feature_max_idx['city'], embedding_dim),\n",
    "        SparseFeat('u_age', feature_max_idx['age'], embedding_dim),\n",
    "        SparseFeat('u_gender', feature_max_idx['gender'], embedding_dim),\n",
    "        SparseFeat('residence',feature_max_idx[\"residence\"],embedding_dim),\n",
    "        SparseFeat('city_rank',feature_max_idx[\"city_rank\"],embedding_dim),\n",
    "        SparseFeat('series_dev',feature_max_idx[\"series_dev\"],embedding_dim),\n",
    "        SparseFeat('series_group',feature_max_idx[\"series_group\"],embedding_dim),\n",
    "        SparseFeat('emui_dev',feature_max_idx[\"emui_dev\"],embedding_dim),\n",
    "        SparseFeat('device_name',feature_max_idx[\"device_name\"],embedding_dim),\n",
    "        SparseFeat('device_size',feature_max_idx[\"device_size\"],embedding_dim),\n",
    "        SparseFeat('net_type_list',feature_max_idx[\"net_type\"],embedding_dim),\n",
    "       # DenseFeat('hist_len', 1,),   \n",
    "                        VarLenSparseFeat(SparseFeat('short_movie_id', feature_max_idx['adv_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN_short, 'mean',\n",
    "                                         'short_sess_length'),\n",
    "                        VarLenSparseFeat(SparseFeat('prefer_movie_id', feature_max_idx['adv_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN_prefer, 'mean',\n",
    "                                         'prefer_sess_length'),\n",
    "                        VarLenSparseFeat(SparseFeat('short_genres', feature_max_idx['creat_type_cd'], embedding_dim,\n",
    "                                                    embedding_name=\"genres\"), SEQ_LEN_short, 'mean',\n",
    "                                         'short_sess_length'),\n",
    "                        VarLenSparseFeat(SparseFeat('prefer_genres', feature_max_idx['creat_type_cd'], embedding_dim,\n",
    "                                                    embedding_name=\"genres\"), SEQ_LEN_prefer, 'mean',\n",
    "                                         'prefer_sess_length'),]\n",
    "#                         ]\n",
    "#  \"task_id\":train_task_id,\n",
    "#         \"create_type\":train_create_type,\n",
    "#         \"adv_prim\":train_adv_prim,\n",
    "#         \"inter_tyepe\":train_inter_tyepe_list,\n",
    "#         \"slot_id\":train_slot_id_list,\n",
    "#         \"spread_app_id\":train_spread_app_id_list,\n",
    "#         \"pp_second_id\":train_app_second_id_list\n",
    "\n",
    "\n",
    "\n",
    "item_feature_columns = [SparseFeat('adv_id', feature_max_idx['adv_id'], embedding_dim),\n",
    "# SparseFeat('task_id', feature_max_idx['task_id'], embedding_dim),\n",
    "# SparseFeat('adv_prim', feature_max_idx['adv_prim_id'], embedding_dim),\n",
    "# SparseFeat('inter_tyepe', feature_max_idx['inter_type_cd'], embedding_dim),\n",
    "# SparseFeat('slot_id', feature_max_idx['slot_id'], embedding_dim),\n",
    "# SparseFeat('spread_app_id', feature_max_idx['spread_app_id'], embedding_dim),\n",
    "# SparseFeat('pp_second_id', feature_max_idx['app_second_class'], embedding_dim),\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "from collections import Counter\n",
    "train_counter = Counter(train_model_input['adv_id'])\n",
    "item_count = [train_counter.get(i,0) for i in range(item_feature_columns[0].vocabulary_size)]\n",
    "sampler_config = NegativeSampler('frequency',num_sampled=255,item_name=\"adv_id\",item_count=item_count)\n",
    "\n",
    "# 3.Define Model and train\n",
    "\n",
    "import tensorflow as tf\n",
    "if tf.__version__ >= '2.0.0':\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "else:\n",
    "    K.set_learning_phase(True)\n",
    "\n",
    "model = SDM(user_feature_columns, item_feature_columns, history_feature_list=['movie_id','genres'],\n",
    "            units=embedding_dim, sampler_config=sampler_config )\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n",
    "\n",
    "history = model.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=64, epochs=20, verbose=1, validation_split=0.0, )\n",
    "full_data=train_set+test_set\n",
    "full_data_input,full_data_label=gen_model_input(full_data)\n",
    "# 4. Generate user features for testing and full item features for retrieval\n",
    "user_embs, doc_embs = get_embeddings(model, full_data_input, user_idx_2_rawid, doc_idx_2_rawid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_columns = [\n",
    "    SparseFeat('user_id', feature_max_idx['user_id'], embedding_dim),\n",
    "    SparseFeat('u_city', feature_max_idx['city'], embedding_dim),\n",
    "        SparseFeat('u_age', feature_max_idx['age'], embedding_dim),\n",
    "        SparseFeat('u_gender', feature_max_idx['gender'], embedding_dim),\n",
    "        SparseFeat('residence',feature_max_idx[\"residence\"],embedding_dim),\n",
    "        SparseFeat('city_rank',feature_max_idx[\"city_rank\"],embedding_dim),\n",
    "        SparseFeat('series_dev',feature_max_idx[\"series_dev\"],embedding_dim),\n",
    "        SparseFeat('series_group',feature_max_idx[\"series_group\"],embedding_dim),\n",
    "        SparseFeat('emui_dev',feature_max_idx[\"emui_dev\"],embedding_dim),\n",
    "        SparseFeat('device_name',feature_max_idx[\"device_name\"],embedding_dim),\n",
    "        SparseFeat('device_size',feature_max_idx[\"device_size\"],embedding_dim),\n",
    "        SparseFeat('net_type_list',feature_max_idx[\"net_type\"],embedding_dim),\n",
    "        #DenseFeat('hist_len', 1,),   \n",
    "                        VarLenSparseFeat(SparseFeat('short_movie_id', feature_max_idx['adv_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN_short, 'mean',\n",
    "                                         'short_sess_length'),\n",
    "                        VarLenSparseFeat(SparseFeat('prefer_movie_id', feature_max_idx['adv_id'], embedding_dim,\n",
    "                                                    embedding_name=\"movie_id\"), SEQ_LEN_prefer, 'mean',\n",
    "                                         'prefer_sess_length'),\n",
    "                        VarLenSparseFeat(SparseFeat('short_genres', feature_max_idx['creat_type_cd'], embedding_dim,\n",
    "                                                    embedding_name=\"genres\"), SEQ_LEN_short, 'mean',\n",
    "                                         'short_sess_length'),\n",
    "                        VarLenSparseFeat(SparseFeat('prefer_genres', feature_max_idx['creat_type_cd'], embedding_dim,\n",
    "                                                    embedding_name=\"genres\"), SEQ_LEN_prefer, 'mean',\n",
    "                                         'prefer_sess_length')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_feature_columns = [SparseFeat('adv_id', feature_max_idx['adv_id'], embedding_dim)\n",
    "# SparseFeat('task_id', feature_max_idx['task_id'], embedding_dim),\n",
    "# SparseFeat('adv_prim', feature_max_idx['adv_prim_id'], embedding_dim),\n",
    "# SparseFeat('inter_tyepe', feature_max_idx['inter_type_cd'], embedding_dim),\n",
    "# SparseFeat('slot_id', feature_max_idx['slot_id'], embedding_dim),\n",
    "# SparseFeat('spread_app_id', feature_max_idx['spread_app_id'], embedding_dim),\n",
    "# SparseFeat('pp_second_id', feature_max_idx['app_second_class'], embedding_dim),\n",
    "\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\deepmatch\\layers\\sequence.py:35: BasicLSTMCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\deepmatch\\layers\\sequence.py:65: MultiRNNCell.__init__ (from tensorflow.python.keras.layers.legacy_rnn.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\deepmatch\\layers\\sequence.py:78: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:738: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v1.py:67: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\Youtube\\lib\\site-packages\\tensorflow\\python\\ops\\linalg\\linear_operator_lower_triangular.py:151: calling LinearOperator.__init__ (from tensorflow.python.ops.linalg.linear_operator) with graph_parents is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not pass `graph_parents`.  They will  no longer be used.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "train_counter = Counter(train_model_input['adv_id'])\n",
    "item_count = [train_counter.get(i,0) for i in range(item_feature_columns[0].vocabulary_size)]\n",
    "sampler_config = NegativeSampler('frequency',num_sampled=255,item_name=\"adv_id\",item_count=item_count)\n",
    "\n",
    "# 3.Define Model and train\n",
    "\n",
    "import tensorflow as tf\n",
    "if tf.__version__ >= '2.0.0':\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "else:\n",
    "    K.set_learning_phase(True)\n",
    "\n",
    "model = SDM(user_feature_columns, item_feature_columns, history_feature_list=['movie_id','genres'],\n",
    "            units=embedding_dim, sampler_config=sampler_config )\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=sampledsoftmaxloss)\n",
    "\n",
    "history = model.fit(train_model_input, train_label,  # train_label,\n",
    "                    batch_size=64, epochs=20, verbose=1, validation_split=0.0, )\n",
    "full_data=train_set+test_set\n",
    "full_data_input,full_data_label=gen_model_input(full_data)\n",
    "# 4. Generate user features for testing and full item features for retrieval\n",
    "user_embs, doc_embs = get_embeddings(model, full_data_input, user_idx_2_rawid, doc_idx_2_rawid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "colab_MovieLen1M_SDM.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.13 ('Youtube')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca454ec5ac04712718d68f09ee0cbc78a463fd81d8627a8a25a13c0a20f92098"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
